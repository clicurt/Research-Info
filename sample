#include "opencv2/opencv_modules.hpp"
#include "opencv2/core/core.hpp"
#include "opencv2/features2d/features2d.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/xfeatures2d/nonfree.hpp"
#include "opencv2/imgproc/imgproc.hpp"
#include <stdio.h>
#include <iostream>

using namespace std;
using namespace cv;
using namespace cv::xfeatures2d;

int main(int argc, char **argv)
{
    // Mat img_1, img_2;
    Mat img;

    // Load image in grayscale format
    img = imread("/home/ub/Pictures/puppy_100.jpg", IMREAD_GRAYSCALE);

    // Rotate the input image without loosing the corners
    // Point center = Point(img_1.cols / 2, img_1.rows / 2);
    // double angle = 45, scale = 1;
    // Mat rot = getRotationMatrix2D(center, angle, scale);
    // Rect bbox = cv::RotatedRect(center, img_1.size(), angle).boundingRect();
    // rot.at<double>(0,2) += bbox.width/2.0 - center.x;
    // rot.at<double>(1,2) += bbox.height/2.0 - center.y;
    // warpAffine(img_1, img_2, rot, bbox.size());

    // SIFT feature detector
    // SiftFeatureDetector detector;
    string algo = "SURF";

    Mat descriptors;

    if (algo == "SIFT")
    {
        Ptr<SIFT> ptrSift = SIFT::create();
        std::vector<KeyPoint> keypoints;
        ptrSift->detect(img, keypoints);
        Ptr<SiftDescriptorExtractor> sift = SiftDescriptorExtractor::create();
        sift->compute(img, keypoints, descriptors);
    }

    if (algo == "SURF")
    {
        Ptr<SURF> ptrSurf = SURF::create();
        std::vector<KeyPoint> keypoints;
        ptrSurf->detect(img, keypoints);
        Ptr<SurfDescriptorExtractor> surf = SurfDescriptorExtractor::create();
        surf->compute(img, keypoints, descriptors);
    }

    if (algo == "ORB")
    {
        Ptr<ORB> ptrOrb = ORB::create();
        std::vector<KeyPoint> keypoints;
        ptrOrb->detect(img, keypoints);
        ptrOrb->detectAndCompute(img, noArray(), keypoints, descriptors);
    }

    if (algo == "BRIEF")
    {
        Ptr<ORB> ptrOrb = ORB::create();
        std::vector<KeyPoint> keypoints;
        ptrOrb->detect(img, keypoints);
        // auto featureExtractor = BriefDescriptorExtractor
        
        // ptrOrb->detectAndCompute(img, noArray(), keypoints, descriptors);
    }

    cout <<  "Descriptors (" << descriptors.rows << "," << descriptors.cols << ")" << endl;

    waitKey(0);
    return 0;
}



if (descriptors.isContinuous()) {
    descriptorBinary.assign(descriptors.data, descriptors.data + descriptors.total()*descriptors.channels());
} else {
    for (int i = 0; i < descriptors.rows; ++i) {
        descriptorBinary.insert(descriptorBinary.end(), descriptors.ptr<uchar>(i), descriptors.ptr<uchar>(i)+descriptors.cols*descriptors.channels());
    }
}

Point center = Point(img.cols / 2, img.rows / 2);
double angle = 45, scale = 1;
Mat rot = getRotationMatrix2D(center, angle, scale);
Rect bbox = cv::RotatedRect(center, img.size(), angle).boundingRect();
rot.at<double>(0,2) += bbox.width/2.0 - center.x;
rot.at<double>(1,2) += bbox.height/2.0 - center.y;
warpAffine(img, img1, rot, bbox.size());

### Flags

int32 FLAGS_logging_level: 3: Log display level [0 ~ 255] No log display at 255
string FLAGS_image_path: "examples / media / COCO_val2014_000000000192.jpg": Image path
string FLAGS_model_pose: "COCO": Model type (choose from COCO, MPI, MPI_4_layers)
string FLAGS_model_folder: "models /": Path of Model (COCO / MPI) folder
string FLAGS_net_resolution: "656x368": The resolution of the processed image, a multiple of 16.
string FLAGS_resolution: "1280x720": Display image resolution, "-1x-1 \" forcibly specify default.
int32 FLAGS_num_gpu_start: "0": GPU number used for calculation (can be selected if there are multiple GPUs)
double FLAGS_scale_gap: "0.3": Scale ratio of input image and output image.
int32 FLAGS_num_scales: "1": Scale averaging count? (Unclear, corrected later)
double FLAGS_alpha_pose :: Alpha blend value of the skeleton of the output image [0 ~ 1]


### Pose Extraction steps

// Step 1 - Set logging level
 op::check(0 <= FLAGS_logging_level && FLAGS_logging_level <= 255, "Wrong logging_level value.",
              __LINE__, __FUNCTION__, __FILE__);

// Step 2 - Read Google flags (user defined configuration)
    // outputSize
    const auto outputSize = op::flagsToPoint(FLAGS_output_resolution, "-1x-1");
    // netInputSize
    const auto netInputSize = op::flagsToPoint(FLAGS_net_resolution, "-1x368");
    // poseModel
    const auto poseModel = op::flagsToPoseModel(FLAGS_model_pose);
    
// Step 3 - Initialize all required classes
    op::ScaleAndSizeExtractor scaleAndSizeExtractor(netInputSize, outputSize, FLAGS_scale_number, FLAGS_scale_gap);
    op::CvMatToOpInput cvMatToOpInput;
    op::CvMatToOpOutput cvMatToOpOutput;
    op::PoseExtractorCaffe poseExtractorCaffe{poseModel, FLAGS_model_folder,
                                              FLAGS_num_gpu_start, {}, op::ScaleMode::ZeroToOne, enableGoogleLogging};
    op::PoseCpuRenderer poseRenderer{poseModel, (float)FLAGS_render_threshold, !FLAGS_disable_blending,
                                     (float)FLAGS_alpha_pose};
    op::OpOutputToCvMat opOutputToCvMat;
    op::FrameDisplayer frameDisplayer{"OpenPose Tutorial - Example 1", outputSize};
    
// Step 4 - Initialize resources on desired thread (in this case single thread, i.e. we init resources here)
    poseExtractorCaffe.initializationOnThread();
    poseRenderer.initializationOnThread();

/ ------------------------- POSE ESTIMATION AND RENDERING -------------------------
// Step 1 - Read and load image, error if empty (possibly wrong path)
// Alternative(Opencv function): cv::imread(FLAGS_image_path, CV_LOAD_IMAGE_COLOR);
    cv::Mat inputImage = op::loadImage(FLAGS_image_path, CV_LOAD_IMAGE_COLOR);
    if(inputImage.empty())
        op::error("Could not open or find the image: " + FLAGS_image_path, __LINE__, __FUNCTION__, __FILE__);
    const op::Point<int> imageSize{inputImage.cols, inputImage.rows};

// Step 2 - Get desired scale sizes
    std::vector<double> scaleInputToNetInputs;
    std::vector<op::Point<int>> netInputSizes;
    double scaleInputToOutput;
    op::Point<int> outputResolution;
    std::tie(scaleInputToNetInputs, netInputSizes, scaleInputToOutput, outputResolution)
        = scaleAndSizeExtractor.extract(imageSize);

// Step 3 - Format input image to OpenPose input and output formats
    const auto netInputArray = cvMatToOpInput.createArray(inputImage, scaleInputToNetInputs, netInputSizes);
    auto outputArray = cvMatToOpOutput.createArray(inputImage, scaleInputToOutput, outputResolution);

// Step 4 - Estimate poseKeypoints
    poseExtractorCaffe.forwardPass(netInputArray, imageSize, scaleInputToNetInputs);
    const auto poseKeypoints = poseExtractorCaffe.getPoseKeypoints();

// Step 5 - Render poseKeypoints
    poseRenderer.renderPose(outputArray, poseKeypoints, scaleInputToOutput);

// Step 6 - OpenPose output format to cv::Mat
    auto outputImage = opOutputToCvMat.formatToCvMat(outputArray);
    
// ------------------------- SHOWING RESULT AND CLOSING -------------------------
// Step 1 - Show results
    frameDisplayer.displayFrame(outputImage, 0); // Alternative(Opencv function): cv::imshow(outputImage) + cv::waitKey(0)
// Step 2 - Logging information message
    op::log("Example 1 successfully finished.", op::Priority::High);
   
